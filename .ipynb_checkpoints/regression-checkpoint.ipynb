{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project-6 Shuzo Katayama, October 25 2020\n",
    "\n",
    "This prject takes personal data from a variety of American beneficiaries of health insurance, including their age, sex, bmi, number of children, and if they are a smoker, and also provides their individual medical costs billed by health insurance. The task of these models will be to predict the insurance cost of an individual depending on the afforementioned factors\n",
    "\n",
    "Data from: https://www.kaggle.com/mirichoi0218/insurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>smoker</th>\n",
       "      <th>region</th>\n",
       "      <th>charges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>female</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>southwest</td>\n",
       "      <td>16884.92400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>male</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1725.55230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>4449.46200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>male</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>21984.47061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>male</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>3866.85520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>male</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>northwest</td>\n",
       "      <td>10600.54830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>northeast</td>\n",
       "      <td>2205.98080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>female</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southeast</td>\n",
       "      <td>1629.83350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>female</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>southwest</td>\n",
       "      <td>2007.94500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>female</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>yes</td>\n",
       "      <td>northwest</td>\n",
       "      <td>29141.36030</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     sex     bmi  children smoker     region      charges\n",
       "0      19  female  27.900         0    yes  southwest  16884.92400\n",
       "1      18    male  33.770         1     no  southeast   1725.55230\n",
       "2      28    male  33.000         3     no  southeast   4449.46200\n",
       "3      33    male  22.705         0     no  northwest  21984.47061\n",
       "4      32    male  28.880         0     no  northwest   3866.85520\n",
       "...   ...     ...     ...       ...    ...        ...          ...\n",
       "1333   50    male  30.970         3     no  northwest  10600.54830\n",
       "1334   18  female  31.920         0     no  northeast   2205.98080\n",
       "1335   18  female  36.850         0     no  southeast   1629.83350\n",
       "1336   21  female  25.800         0     no  southwest   2007.94500\n",
       "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
       "\n",
       "[1338 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('insurance.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will clean the data. Namely, I will make the values under 'sex' to be 0 for male and 1 for female, and the values under 'smoker' to be 0 for no and 1 for yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'male' to 0, 'female' to 1 \n",
    "for item in a:\n",
    "    if item[1] == \"male\":\n",
    "        item[1] = 0\n",
    "    else:\n",
    "        item[1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'no' to 0, 'yes' to 1 \n",
    "for item in a:\n",
    "    if item[4] == \"no\":\n",
    "        item[4] = 0\n",
    "    else:\n",
    "        item[4] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will split the data into training and testing sets using train_test_split, and standardise the data using StandardScaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into features (X) and targets(y)\n",
    "X = a[:, [0,1,2,3,4]]\n",
    "X = X.astype('float64')\n",
    "y = a[:, 6]\n",
    "y = y.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaler code from regression example\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "sc_y = StandardScaler()\n",
    "sc.fit(y_train[:, np.newaxis])\n",
    "y_train_std = sc.transform(y_train[:, np.newaxis]).flatten()\n",
    "y_test_std = sc.transform(y_test[:, np.newaxis]).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model that will be trained with this data will be the Random Forest Regressor, built into scikit learn. This model trains a large number of small and 'sloppy' trees and aggregate their results into a prediction. Instead of taking a majority vote like in a random forest classifier, the regressor will average the outputs since it is providing a continuous answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(n_estimators=1000, n_jobs=-1, random_state=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model\n",
    "est = RandomForestRegressor(n_estimators=1000, criterion='mse', random_state=1, n_jobs=-1)\n",
    "est.fit(X_train_std, y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 0.023, test: 0.174\n",
      "R^2 train: 0.977, test: 0.820\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "y_train_pred = est.predict(X_train_std)\n",
    "y_test_pred = est.predict(X_test_std)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train_std, y_train_pred),\n",
    "        mean_squared_error(y_test_std, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train_std, y_train_pred),\n",
    "        r2_score(y_test_std, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Regression model has an R<sup>2</sup> of 0.977, which means that the model was able to fit the data very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next model i will train will be the symbolic regressor, using gplearn. The symbolic regressor uses a genetic algorithm to create a mathetmatical function, based off of the primordial ooze it was given (a variety of simple mathematical functions). From this genetic algorithm, the symbolic regressor will create a function that fits the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gplearn.genetic import SymbolicRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    |   Population Average    |             Best Individual              |\n",
      "---- ------------------------- ------------------------------------------ ----------\n",
      " Gen   Length          Fitness   Length          Fitness      OOB Fitness  Time Left\n",
      "   0    14.29          4.58055        3         0.455755              N/A     51.59s\n",
      "   1    11.51          1.23798        1         0.455755              N/A     52.09s\n",
      "   2     9.83          1.19294        3         0.455755              N/A     49.87s\n",
      "   3     9.24          1.17491       11         0.431811              N/A     48.09s\n",
      "   4     7.47         0.931876        8          0.43673              N/A     46.33s\n",
      "   5     5.98          2.35363        8          0.43673              N/A     45.52s\n",
      "   6     4.75          1.02175        8          0.43673              N/A     44.75s\n",
      "   7     3.41         0.934759       16         0.407669              N/A     42.08s\n",
      "   8     3.20          0.90123       19         0.407669              N/A     42.11s\n",
      "   9     3.69         0.972158       18         0.407669              N/A     42.46s\n",
      "  10     4.38         0.930235       16         0.407669              N/A     42.62s\n",
      "  11     5.82         0.865644       25         0.407669              N/A     43.36s\n",
      "  12     8.42         0.842343       16         0.407669              N/A     43.90s\n",
      "  13    10.70         0.825706       17         0.405482              N/A     45.38s\n",
      "  14    13.47         0.909303       17         0.397978              N/A     46.54s\n",
      "  15    15.72         0.755438       27         0.355615              N/A     47.42s\n",
      "  16    16.39         0.785932       30         0.355615              N/A     47.07s\n",
      "  17    16.24         0.753683       26         0.355288              N/A     46.90s\n",
      "  18    17.17         0.764107       34         0.355288              N/A     50.13s\n",
      "  19    17.82         0.750625       24         0.355615              N/A     45.80s\n",
      "  20    20.14          0.79683       26         0.355615              N/A     47.36s\n",
      "  21    23.03         0.753161       31          0.35077              N/A     48.37s\n",
      "  22    26.72         0.618756       30         0.330033              N/A     50.64s\n",
      "  23    30.49         0.573469       32         0.326638              N/A     50.98s\n",
      "  24    33.49         0.618278       30         0.330033              N/A     53.80s\n",
      "  25    37.23         0.897851       47         0.317102              N/A      1.05m\n",
      "  26    38.39         0.564862       35         0.315397              N/A     58.17s\n",
      "  27    40.08         0.605723       32         0.315397              N/A     59.25s\n",
      "  28    42.34         0.560192       41         0.298406              N/A     57.68s\n",
      "  29    43.17          0.60459       74         0.304322              N/A      1.01m\n",
      "  30    44.24         0.594386       64         0.305642              N/A     54.90s\n",
      "  31    48.33         0.549847       47         0.304103              N/A     56.49s\n",
      "  32    55.02         0.585254       40         0.295863              N/A     57.40s\n",
      "  33    58.31         0.570221       34         0.303672              N/A      1.08m\n",
      "  34    57.50         0.554847       93         0.295015              N/A      1.05m\n",
      "  35    57.22         0.527341       83         0.291985              N/A      1.03m\n",
      "  36    60.29         0.551963       68         0.285302              N/A      1.04m\n",
      "  37    64.52         0.591253       73         0.285302              N/A      1.04m\n",
      "  38    67.90         0.556145       35         0.285302              N/A      1.14m\n",
      "  39    71.95         0.508853      114         0.283081              N/A      1.19m\n",
      "  40    75.86         0.506944       51         0.281443              N/A      1.15m\n",
      "  41    75.19         0.558555      144         0.274474              N/A      1.07m\n",
      "  42    78.18         0.540685       77         0.274474              N/A      1.13m\n",
      "  43    82.20         0.626189       84         0.274378              N/A      1.10m\n",
      "  44    87.56         0.568446       54         0.266143              N/A      1.09m\n",
      "  45    88.57         0.524622       79         0.266143              N/A      1.10m\n",
      "  46    90.12          1.08616       99         0.265432              N/A      1.07m\n",
      "  47    91.06          2.69851      153         0.260678              N/A      1.16m\n",
      "  48    94.97         0.495892      116           0.2585              N/A      1.07m\n",
      "  49    94.33         0.483428      113           0.2585              N/A      1.07m\n",
      "  50    94.89         0.457447      124         0.260303              N/A      1.04m\n",
      "  51    91.08         0.493611      100         0.254165              N/A     57.24s\n",
      "  52    90.14         0.505827       91         0.252508              N/A     57.80s\n",
      "  53    88.62           7.8225      218         0.254165              N/A     54.10s\n",
      "  54    88.22         0.471733       96         0.252126              N/A     53.25s\n",
      "  55    89.99          0.44812       85         0.251498              N/A     58.03s\n",
      "  56    92.60         0.462628      132         0.249136              N/A     53.87s\n",
      "  57    95.79         0.613242      119         0.249136              N/A     50.84s\n",
      "  58    94.55         0.470974       82         0.248876              N/A     50.24s\n",
      "  59    90.53         0.480185      114         0.250386              N/A     48.79s\n",
      "  60    84.45         0.467655       61         0.247194              N/A     42.95s\n",
      "  61    78.31         0.511967       51         0.248379              N/A     44.01s\n",
      "  62    74.29         0.492696       58         0.248348              N/A     39.92s\n",
      "  63    72.77         0.515046       66         0.248348              N/A     39.52s\n",
      "  64    71.20         0.533858       90         0.228975              N/A     38.46s\n",
      "  65    71.27         0.471095       84         0.228975              N/A     35.92s\n",
      "  66    71.95         0.505176       97         0.228975              N/A     35.25s\n",
      "  67    72.51         0.537437      150         0.228975              N/A     33.38s\n",
      "  68    73.04          0.57156      108         0.228975              N/A     34.84s\n",
      "  69    76.75         0.519625       96         0.228975              N/A     32.65s\n",
      "  70    82.36         0.533852       69            0.228              N/A     33.03s\n",
      "  71    91.98         0.534957       84         0.228213              N/A     33.87s\n",
      "  72   100.54         0.508106      100         0.224925              N/A     35.13s\n",
      "  73   107.48         0.465721      120         0.224925              N/A     35.50s\n",
      "  74   112.59         0.543797      120         0.224925              N/A     33.96s\n",
      "  75   116.12         0.523227       85         0.220073              N/A     32.87s\n",
      "  76   117.00         0.468783       82         0.220073              N/A     34.19s\n",
      "  77   113.27          0.59802      102         0.220336              N/A     27.42s\n",
      "  78   111.60         0.502178      126         0.220336              N/A     28.47s\n",
      "  79   105.42         0.579565      127         0.216817              N/A     26.42s\n",
      "  80   105.03         0.507629      124         0.216817              N/A     25.99s\n",
      "  81   104.25         0.631356      256         0.222133              N/A     25.14s\n",
      "  82   105.92         0.566933      101         0.222945              N/A     21.64s\n",
      "  83   105.46         0.553057      103         0.222642              N/A     20.17s\n",
      "  84   103.25         0.623464      264         0.215787              N/A     18.72s\n",
      "  85   103.11         0.529116      100         0.220182              N/A     16.41s\n",
      "  86   105.91         0.528206      120         0.217142              N/A     15.25s\n",
      "  87   106.55         0.511185      111         0.213772              N/A     14.33s\n",
      "  88   103.20         0.496377      111         0.213772              N/A     13.05s\n",
      "  89   102.14         0.540496      104         0.213711              N/A     11.65s\n",
      "  90   104.22         0.496684      181         0.213772              N/A     10.58s\n",
      "  91   105.73           0.5212      164         0.213772              N/A     11.66s\n",
      "  92   107.94         0.579183      199         0.213628              N/A      8.32s\n",
      "  93   108.26         0.636371      133         0.208907              N/A      7.18s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  94   114.42         0.465636      103         0.208658              N/A      6.19s\n",
      "  95   117.62         0.452941      125         0.208525              N/A      5.00s\n",
      "  96   118.55         0.511143      199         0.208658              N/A      3.80s\n",
      "  97   121.99         0.533269      179         0.204973              N/A      2.55s\n",
      "  98   124.05         0.488259      165         0.205755              N/A      1.30s\n",
      "  99   127.01         0.469231      160         0.205755              N/A      0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SymbolicRegressor(function_set=('add', 'sub', 'mul', 'div', 'sqrt', 'log',\n",
       "                                'abs', 'neg', 'inv', 'max', 'min', 'sin', 'cos',\n",
       "                                'tan'),\n",
       "                  generations=100, init_depth=(4, 6), p_crossover=0.3,\n",
       "                  p_hoist_mutation=0.0, p_point_mutation=0.35,\n",
       "                  p_subtree_mutation=0.35, parsimony_coefficient=0.0001,\n",
       "                  random_state=0, stopping_criteria=0.01, tournament_size=5,\n",
       "                  verbose=1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "est = SymbolicRegressor(population_size=1000,\n",
    "                        init_depth=(4,6),\n",
    "                        generations=100, stopping_criteria=0.01,\n",
    "                        p_crossover=0.3, p_subtree_mutation=0.35,\n",
    "                        p_hoist_mutation=0.0, p_point_mutation=0.35,\n",
    "                        max_samples=1.0, verbose=1,\n",
    "                        #const_range=None,\n",
    "                        const_range=(-1.0,1.0),\n",
    "                        tournament_size=5,\n",
    "                        function_set=('add', 'sub', 'mul', 'div', 'sqrt', 'log', \n",
    "                                      'abs', 'neg', 'inv', 'max','min', 'sin', 'cos', 'tan'),\n",
    "                        parsimony_coefficient=0.0001, random_state=0)\n",
    "est.fit(X_train_std, y_train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE train: 0.157, test: 0.173\n",
      "R^2 train: 0.843, test: 0.821\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = est.predict(X_train_std)\n",
    "y_test_pred = est.predict(X_test_std)\n",
    "\n",
    "print('MSE train: %.3f, test: %.3f' % (\n",
    "        mean_squared_error(y_train_std, y_train_pred),\n",
    "        mean_squared_error(y_test_std, y_test_pred)))\n",
    "print('R^2 train: %.3f, test: %.3f' % (\n",
    "        r2_score(y_train_std, y_train_pred),\n",
    "        r2_score(y_test_std, y_test_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Symbolic regressor model has an R<sup>2</sup> of 0.843, which means that this model fit the data worse than the Random Forest regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately, the data presented was able to be accurately modeled (with the random forest regressor having a very good coefficient of determination, and with the symbolic regressor having a good coefficient of determination, that was worse than the random forest model). The data presented, therefore, has a good correlation between its factors (the age, sex, bmi, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
